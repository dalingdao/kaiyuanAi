# main.py
import os
import json
import logging
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
import pandas as pd
import numpy as np
from datetime import datetime

# 初始化日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# 配置参数
CONFIG = {
    "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "local_model_dir": "./my_model_dir",
    "max_new_tokens": 50,
    "max_seq_length": 512,
    "batch_size": 2,
    "training_epochs": 1,
    "rl_rounds": 2,
    "sample_file": "samples.csv",
    "reward_sample_file": "reward_samples.csv",
    "training_log_file": "training_results.json"
}

class TrainingDataset(Dataset):
    def __init__(self, encodings, scores=None):
        self.encodings = encodings
        self.scores = scores

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.scores is not None:
            item['scores'] = torch.tensor(self.scores[idx])
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.training_logs = []
        
    def log(self, logs):
        super().log(logs)
        step_log = {
            "epoch": self.state.epoch,
            "step": self.state.global_step,
            "timestamp": datetime.now().isoformat(),
            "metrics": logs.copy()
        }
        self.training_logs.append(step_log)
        logger.info(f"Training Step {self.state.global_step} - Loss: {logs.get('loss', 'N/A'):.4f}")

class ModelTrainer:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(CONFIG['local_model_dir'])
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        model_config = {
            "pad_token_id": self.tokenizer.eos_token_id,
            "attention_mask": True
        }
        
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            CONFIG['local_model_dir'], 
            **model_config
        ).to(self.device)
        
        self.reward_model = AutoModelForCausalLM.from_pretrained(
            CONFIG['local_model_dir'],
            **model_config
        ).to(self.device)
    
    def _save_training_logs(self, logs, model_type):
        log_entry = {
            "model_type": model_type,
            "start_time": datetime.now().isoformat(),
            "config": CONFIG,
            "logs": logs,
            "end_time": datetime.now().isoformat()
        }
        
        if os.path.exists(CONFIG['training_log_file']):
            with open(CONFIG['training_log_file'], 'r') as f:
                existing_logs = json.load(f)
            existing_logs.append(log_entry)
            with open(CONFIG['training_log_file'], 'w') as f:
                json.dump(existing_logs, f, indent=2)
        else:
            with open(CONFIG['training_log_file'], 'w') as f:
                json.dump([log_entry], f, indent=2)
    
    def train_policy(self, dataset):
        logger.info("Starting policy model training...")
        
        training_args = TrainingArguments(
            output_dir='./policy_results',
            num_train_epochs=CONFIG['training_epochs'],
            per_device_train_batch_size=CONFIG['batch_size'],
            logging_dir='./policy_logs',
            report_to="none",
            remove_unused_columns=True,
            logging_steps=1  # 每个step都记录
        )
        
        trainer = CustomTrainer(
            model=self.policy_model,
            args=training_args,
            train_dataset=dataset,
        )
        
        trainer.train()
        self.policy_model.save_pretrained("./policy_model")
        self._save_training_logs(trainer.training_logs, "policy_model")
        logger.info("Policy model training completed.")
    
    def train_reward(self, dataset):
        logger.info("Starting reward model training...")
        
        training_args = TrainingArguments(
            output_dir='./reward_results',
            num_train_epochs=CONFIG['training_epochs'],
            per_device_train_batch_size=CONFIG['batch_size'],
            logging_dir='./reward_logs',
            report_to="none",
            remove_unused_columns=True,
            logging_steps=1
        )
        
        trainer = CustomTrainer(
            model=self.reward_model,
            args=training_args,
            train_dataset=dataset,
        )
        
        trainer.train()
        self.reward_model.save_pretrained("./reward_model")
        self._save_training_logs(trainer.training_logs, "reward_model")
        logger.info("Reward model training completed.")

class RLAgent:
    def __init__(self, trainer):
        self.trainer = trainer
        self.generated_samples = []
    
    def generate_question(self):
        return "Explain machine learning in simple terms."
    
    def get_reward(self, text):
        inputs = self.trainer.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=CONFIG['max_seq_length'],
            padding='max_length'
        ).to(self.trainer.device)
        
        with torch.no_grad():
            outputs = self.trainer.reward_model(**inputs)
        return outputs.logits.mean().item()
    
    def _save_rl_logs(self):
        rl_logs = {
            "rl_rounds": CONFIG['rl_rounds'],
            "generated_samples": self.generated_samples,
            "best_sample": max(self.generated_samples, key=lambda x: x['avg_score'])
        }
        
        with open("rl_results.json", 'w') as f:
            json.dump(rl_logs, f, indent=2)
    
    def run_rl_cycle(self):
        logger.info("Starting RL training...")
        rl_logs = []
        
        for round_num in range(CONFIG['rl_rounds']):
            round_log = {
                "round": round_num + 1,
                "samples": [],
                "start_time": datetime.now().isoformat()
            }
            
            question = self.generate_question()
            answers = []
            scores = []
            
            for attempt in range(2):
                attempt_log = {
                    "attempt": attempt + 1,
                    "question": question,
                    "start_time": datetime.now().isoformat()
                }
                
                inputs = self.trainer.tokenizer(
                    question, 
                    return_tensors="pt", 
                    max_length=CONFIG['max_seq_length'],
                    truncation=True
                ).to(self.trainer.device)
                
                with torch.no_grad():
                    generated = self.trainer.policy_model.generate(
                        **inputs,
                        max_new_tokens=CONFIG['max_new_tokens'],
                        pad_token_id=self.trainer.tokenizer.eos_token_id,
                        attention_mask=inputs['attention_mask']
                    )
                
                answer = self.trainer.tokenizer.decode(
                    generated[0], 
                    skip_special_tokens=True
                )
                score = self.get_reward(answer)
                
                attempt_log.update({
                    "answer": answer,
                    "score": score,
                    "end_time": datetime.now().isoformat()
                })
                answers.append(answer)
                scores.append(score)
                round_log["samples"].append(attempt_log)
                logger.info(f"RL Round {round_num+1} Attempt {attempt+1} - Score: {score:.2f}")
            
            avg_score = np.mean(scores)
            round_log.update({
                "avg_score": avg_score,
                "end_time": datetime.now().isoformat()
            })
            self.generated_samples.append({
                "question": question,
                "answers": answers,
                "avg_score": avg_score
            })
            rl_logs.append(round_log)
            logger.info(f"RL Round {round_num+1} Completed - Avg Score: {avg_score:.2f}")
        
        self._save_rl_logs()
        best_sample = max(self.generated_samples, key=lambda x: x['avg_score'])
        pd.DataFrame([best_sample]).to_csv(
            CONFIG['reward_sample_file'], 
            mode='a', 
            header=not os.path.exists(CONFIG['reward_sample_file'])
        )
        logger.info("RL cycle completed. Best sample saved.")

def main():
    trainer = ModelTrainer()
    rl_agent = RLAgent(trainer)
    
    try:
        if os.path.exists(CONFIG['reward_sample_file']):
            logger.info("Found existing reward samples, training reward model...")
            reward_data = pd.read_csv(CONFIG['reward_sample_file'])
            reward_texts = [q + " " + a for q, alist in zip(reward_data['question'], reward_data['answers'].apply(eval)) for a in alist]
            
            reward_encodings = trainer.tokenizer(
                reward_texts, 
                truncation=True, 
                padding='max_length',
                max_length=CONFIG['max_seq_length']
            )
            
            reward_dataset = TrainingDataset(reward_encodings)
            trainer.train_reward(reward_dataset)
        
        if os.path.exists(CONFIG['sample_file']):
            logger.info("Loading training samples...")
            df = pd.read_csv(CONFIG['sample_file'])
            filtered_df = df[df['score'] > 1]
            
            if not filtered_df.empty:
                logger.info(f"Training policy model with {len(filtered_df)} samples")
                encodings = trainer.tokenizer(
                    filtered_df['text'].tolist(), 
                    truncation=True, 
                    padding='max_length',
                    max_length=CONFIG['max_seq_length']
                )
                
                train_dataset = TrainingDataset(encodings)
                trainer.train_policy(train_dataset)
        
        rl_agent.run_rl_cycle()
        
    except Exception as e:
        logger.error(f"Error in main process: {str(e)}")
        raise

if __name__ == "__main__":
    logger.info("Script started")
    
    if not os.path.exists(CONFIG['local_model_dir']):
        from modelscope import snapshot_download
        snapshot_download(CONFIG['model_name'], local_dir=CONFIG['local_model_dir'])
    
    main()
    logger.info("Script completed successfully")
